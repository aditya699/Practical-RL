{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd73feb1-7ba8-4206-9796-0aca06fe2a91",
   "metadata": {},
   "source": [
    "### 🧠 Decoder‑Only Transformer Architecture\n",
    "\n",
    "![Decoder‑Only Transformer Diagram](https://waylandzhang.github.io/en/images/decoder-only-transformer.jpg)\n",
    "\n",
    "**Figure:** A GPT‑style stack of decoder blocks:\n",
    "- **Input tokens** → Embedding + positional encoding\n",
    "- **Repeated blocks**: masked self-attention + feed‑forward + layer norms + residuals\n",
    "- **Final linear & softmax** → predict next-token logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb7d14-05a8-4ea6-903b-ade6067a2347",
   "metadata": {},
   "source": [
    "# Load the Dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a85b081-a8f9-4cf1-a953-057b767a0b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1115394\n",
      "First 500 characters:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "print(\"Dataset length:\", len(text))\n",
    "print(\"First 500 characters:\\n\", text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203abed-1e84-4dcf-ab75-961e519d3c49",
   "metadata": {},
   "source": [
    "# Bulid Vocab\n",
    "# 🧠 What is Tokenization?\n",
    "# Tokenization is the process of converting text into units (tokens) that a neural network can understand — and then mapping those tokens to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fed924-1776-47df-857d-19ab0cf38f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902617d6-4f6a-4d95-99ad-e5cc4c527039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f12f65c-3b9c-45fc-898b-077f7a1e0193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer dictionaries\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "dict(list(stoi.items())[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1ebce4-11e5-429d-83cd-570bea036125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = {i: ch for ch, i in stoi.items()}  \n",
    "dict(list(itos.items())[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5bbce-c6e6-4971-ac85-052d474f1c4c",
   "metadata": {},
   "source": [
    "#  ✅ In PyTorch, the first real step before model training is:\n",
    "# 🔁 Convert all input data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfb53d6-5a2c-483a-84b3-8a1c0b0109de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset shape: torch.Size([1115394])\n",
      "First 20 tokens: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56])\n"
     ]
    }
   ],
   "source": [
    "# Step 1.2 — Encode entire dataset\n",
    "import torch\n",
    "\n",
    "# Convert the full text to a list of token IDs\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
    "\n",
    "print(\"Tokenized dataset shape:\", data.shape)\n",
    "print(\"First 20 tokens:\", data[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184575e4-a615-4a2e-80b1-9caa48f13abf",
   "metadata": {},
   "source": [
    "# Split the data into train and val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3dc5ff-e97f-4cd9-8545-5e85bf0270d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1003854\n",
      "Val size: 111540\n"
     ]
    }
   ],
   "source": [
    "# Step 1.3 — Split data into train and val\n",
    "split_idx = int(0.9 * len(data))  # 90% train, 10% val\n",
    "\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Val size:\", len(val_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf3e0a0-362e-4390-b70d-6e6f4ae6d34b",
   "metadata": {},
   "source": [
    "# Create Training Batches (x, y pairs)\n",
    "# 🧱 What is batch_size?\n",
    "# batch_size is the number of (x, y) training examples processed in one forward/backward pass of the model.\n",
    "# ⛓ Why batch?\n",
    "# Matrix operations (on GPU) are fastest when done in batches\n",
    "\n",
    "# You get more stable gradients\n",
    "\n",
    "# You reduce variance compared to updating on just one example -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4021b2-40ad-46a2-97fa-f296a19bb813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, block_size=8, batch_size=4):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb11c25-6e21-4c9c-88a9-a202d2d0326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[ 1, 21,  1, 46],\n",
      "        [ 1, 61, 43,  1]])\n",
      "y:\n",
      " tensor([[21,  1, 46, 43],\n",
      "        [61, 43,  1, 46]])\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data, block_size=4, batch_size=2)\n",
    "\n",
    "print(\"x:\\n\", x)\n",
    "print(\"y:\\n\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c283c15-4708-4850-beeb-0d4c6c821d6b",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3397e-4e97-4ede-9308-db6ac7bb4ce3",
   "metadata": {},
   "source": [
    "# 🔹 What exactly is nn.Embedding in PyTorch?\n",
    "# 🧠 High-level idea\n",
    "# nn.Embedding is just a lookup table:\n",
    "# It stores one vector (a list of numbers) for each token ID\n",
    "# 🧱 Summary:\n",
    "# Concept\tExplanation\n",
    "# What it is\tA learnable matrix of shape (vocab_size, emb_dim)\n",
    "# What it does\tLooks up vectors for input token IDs\n",
    "# Is it a neural net layer?\t✅ Yes (has weights, supports backprop)\n",
    "# Why we use it\tTo represent tokens as meaningful vectors\n",
    "\n",
    "# 🧠 So when does it become trainable?\n",
    "# Here’s the secret:\n",
    "\n",
    "# This embedding matrix is a parameter (like any layer's weights)\n",
    "\n",
    "# During training, we calculate loss, then do .backward()\n",
    "\n",
    "# PyTorch computes gradients w.r.t. the rows used (e.g., token 12, 5, 8)\n",
    "\n",
    "# Only those rows get updated in .step()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f41f178-c482-4f8d-bf9c-f0e18727854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs shape: torch.Size([1, 4])\n",
      "Embedded vector shape: torch.Size([1, 4, 32])\n",
      "Embedded vectors:\n",
      " tensor([[[-9.1476e-01,  4.2747e-01,  1.6521e+00, -1.3608e+00, -9.3403e-01,\n",
      "          -2.3145e+00, -8.7017e-01,  2.5174e-01,  1.1242e+00, -1.2016e+00,\n",
      "          -2.4265e-01,  2.0237e+00,  9.0045e-01,  1.5163e+00,  1.5448e+00,\n",
      "           3.3130e-01,  4.2435e-02, -8.5349e-01, -2.4111e-01,  1.0200e+00,\n",
      "           1.0720e+00,  2.3498e+00, -2.0369e-01,  2.0764e-01,  6.8549e-01,\n",
      "           1.9217e-01,  1.6880e+00, -6.8372e-01, -6.5884e-01, -5.8992e-01,\n",
      "          -8.8590e-01, -7.5728e-01],\n",
      "         [ 6.0499e-01,  8.8128e-01, -1.5651e+00, -1.7934e+00,  7.4650e-01,\n",
      "           2.6304e-01,  3.7010e-01, -7.4764e-01,  4.2487e-02,  1.7857e+00,\n",
      "           6.0633e-01, -2.8965e-01, -2.4978e-01, -1.7072e-01, -6.1268e-01,\n",
      "          -1.0081e+00, -7.4238e-02, -2.0559e+00, -3.4736e-01, -8.6163e-01,\n",
      "           9.9268e-01,  4.3336e-01,  8.6749e-01, -2.8927e-01, -1.7951e-01,\n",
      "          -1.3776e+00, -4.8565e-01, -4.3918e-01,  7.2193e-01, -1.0455e-03,\n",
      "           2.2425e+00, -2.5870e-01],\n",
      "         [ 1.7435e-02, -1.8823e-02, -1.2319e+00, -2.0064e-01,  2.3179e+00,\n",
      "           7.5409e-01, -4.5407e-01,  5.5342e-01, -1.3755e+00, -7.5618e-01,\n",
      "          -1.5038e+00,  9.8195e-01,  1.4302e+00,  9.8889e-01, -8.4476e-01,\n",
      "           4.1802e-01,  8.4048e-01,  7.8687e-01,  1.4958e+00, -1.0158e+00,\n",
      "          -2.0246e-01, -9.6747e-01,  1.4234e+00, -3.0872e-01,  6.3919e-01,\n",
      "          -4.0239e-01,  4.8788e-01, -8.6573e-02, -1.1334e+00,  2.3413e-01,\n",
      "          -8.5764e-01, -6.6776e-01],\n",
      "         [ 1.7435e-02, -1.8823e-02, -1.2319e+00, -2.0064e-01,  2.3179e+00,\n",
      "           7.5409e-01, -4.5407e-01,  5.5342e-01, -1.3755e+00, -7.5618e-01,\n",
      "          -1.5038e+00,  9.8195e-01,  1.4302e+00,  9.8889e-01, -8.4476e-01,\n",
      "           4.1802e-01,  8.4048e-01,  7.8687e-01,  1.4958e+00, -1.0158e+00,\n",
      "          -2.0246e-01, -9.6747e-01,  1.4234e+00, -3.0872e-01,  6.3919e-01,\n",
      "          -4.0239e-01,  4.8788e-01, -8.6573e-02, -1.1334e+00,  2.3413e-01,\n",
      "          -8.5764e-01, -6.6776e-01]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Vocabulary size — number of unique tokens\n",
    "vocab_size = 65\n",
    "\n",
    "# Embedding dimension — how big each token's vector should be\n",
    "embedding_dim = 32\n",
    "\n",
    "# Step 1: Create the embedding layer\n",
    "token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Step 2: Create a batch of token IDs\n",
    "x = torch.tensor([[12, 5, 8, 8]])  # shape = (batch_size=1, block_size=4)\n",
    "\n",
    "# Step 3: Apply the embedding layer\n",
    "x_embed = token_embedding_table(x)\n",
    "\n",
    "# Step 4: Print shapes and values\n",
    "print(\"Input token IDs shape:\", x.shape)\n",
    "print(\"Embedded vector shape:\", x_embed.shape)\n",
    "print(\"Embedded vectors:\\n\", x_embed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73386dbc-232c-4e29-b3aa-3e034dcff8c9",
   "metadata": {},
   "source": [
    "# Add Positional Encoding\n",
    "# 🧠 Why?\n",
    "# Transformers have no sense of order — they treat all tokens as a bag of vectors.\n",
    "\n",
    "# But language is ordered:\n",
    "\n",
    "# \"The cat sat\" ≠ \"Sat the cat\"\n",
    "\n",
    "# So we must inject position information into each token’s embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9012ec06-e8e4-4036-a01d-a6ce36ccf4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of table: torch.Size([4, 8])\n",
      "Positional Embedding Table:\n",
      " Parameter containing:\n",
      "tensor([[ 0.1651,  0.5164, -0.5502, -2.0563, -1.3057,  0.7361, -0.8709, -0.7991],\n",
      "        [ 0.5314, -0.9714, -0.9523, -0.2689, -0.1302,  0.2233,  0.9946,  0.9599],\n",
      "        [ 0.0183, -0.2316, -0.4117, -0.0224,  0.4507,  2.8825,  0.1056,  0.1770],\n",
      "        [ 0.0062, -0.9122,  0.8633, -0.1300,  0.1260, -2.3718, -0.6157,  0.7260]],\n",
      "       requires_grad=True)\n",
      "Vector for position 2:\n",
      " tensor([[ 0.0183, -0.2316, -0.4117, -0.0224,  0.4507,  2.8825,  0.1056,  0.1770]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Position vectors:\n",
      " tensor([[ 0.1651,  0.5164, -0.5502, -2.0563, -1.3057,  0.7361, -0.8709, -0.7991],\n",
      "        [ 0.5314, -0.9714, -0.9523, -0.2689, -0.1302,  0.2233,  0.9946,  0.9599],\n",
      "        [ 0.0183, -0.2316, -0.4117, -0.0224,  0.4507,  2.8825,  0.1056,  0.1770],\n",
      "        [ 0.0062, -0.9122,  0.8633, -0.1300,  0.1260, -2.3718, -0.6157,  0.7260]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "position_embedding_table = nn.Embedding(block_size, embedding_dim)\n",
    "print(\"Shape of table:\", position_embedding_table.weight.shape)\n",
    "print(\"Positional Embedding Table:\\n\", position_embedding_table.weight)\n",
    "pos_vector = position_embedding_table(torch.tensor([2]))\n",
    "print(\"Vector for position 2:\\n\", pos_vector)\n",
    "position_ids = torch.arange(block_size)\n",
    "pos_vectors = position_embedding_table(position_ids)\n",
    "print(\"Position vectors:\\n\", pos_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64da19f1-c04e-4db2-871c-d8d188a13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3752,  0.5033,  0.1096, -0.0824,  0.6980,  0.6636, -0.5450,\n",
      "           0.1132],\n",
      "         [-0.0930, -1.5122, -2.2702,  1.1632, -0.0315, -0.6675, -0.1314,\n",
      "           0.8455],\n",
      "         [ 2.1030, -2.1707,  1.8961, -0.2121,  0.4001, -0.0505,  1.2624,\n",
      "           0.4154],\n",
      "         [ 2.1030, -2.1707,  1.8961, -0.2121,  0.4001, -0.0505,  1.2624,\n",
      "           0.4154]],\n",
      "\n",
      "        [[ 0.3501, -1.7663,  1.0883,  0.5273,  1.0713,  1.7445,  0.6715,\n",
      "          -0.6527],\n",
      "         [-0.4822,  0.6016,  0.8966,  0.1061, -0.2813,  0.2576, -0.2173,\n",
      "           0.3970],\n",
      "         [-0.2071,  0.3486, -0.4923,  1.2341, -0.6933, -2.1536, -0.2513,\n",
      "           0.8460],\n",
      "         [-0.8765, -1.7758,  2.3262,  0.2258,  0.4728, -0.2361,  0.2439,\n",
      "          -0.4916]]], grad_fn=<EmbeddingBackward0>)\n",
      "Token embeddings:\n",
      " tensor([[[-0.3752,  0.5033,  0.1096, -0.0824,  0.6980,  0.6636, -0.5450,\n",
      "           0.1132],\n",
      "         [-0.0930, -1.5122, -2.2702,  1.1632, -0.0315, -0.6675, -0.1314,\n",
      "           0.8455],\n",
      "         [ 2.1030, -2.1707,  1.8961, -0.2121,  0.4001, -0.0505,  1.2624,\n",
      "           0.4154],\n",
      "         [ 2.1030, -2.1707,  1.8961, -0.2121,  0.4001, -0.0505,  1.2624,\n",
      "           0.4154]],\n",
      "\n",
      "        [[ 0.3501, -1.7663,  1.0883,  0.5273,  1.0713,  1.7445,  0.6715,\n",
      "          -0.6527],\n",
      "         [-0.4822,  0.6016,  0.8966,  0.1061, -0.2813,  0.2576, -0.2173,\n",
      "           0.3970],\n",
      "         [-0.2071,  0.3486, -0.4923,  1.2341, -0.6933, -2.1536, -0.2513,\n",
      "           0.8460],\n",
      "         [-0.8765, -1.7758,  2.3262,  0.2258,  0.4728, -0.2361,  0.2439,\n",
      "          -0.4916]]], grad_fn=<EmbeddingBackward0>)\n",
      "Positional embeddings:\n",
      " tensor([[[ 0.3941, -0.3778, -0.4057, -0.9342, -1.3242,  0.7197,  0.8428,\n",
      "           1.5240],\n",
      "         [-0.8842,  1.0614,  1.3315, -0.5393,  0.5199, -0.4273, -0.5576,\n",
      "           0.6584],\n",
      "         [-1.6838,  1.5486, -0.1241, -0.7470, -0.0380,  2.1499,  0.2778,\n",
      "           0.3548],\n",
      "         [ 1.2008, -0.9429,  0.4536, -0.5976,  1.3221, -0.9845, -0.2370,\n",
      "           0.1800]]], grad_fn=<UnsqueezeBackward0>)\n",
      "Final input to transformer (x_embed):\n",
      " tensor([[[ 0.0189,  0.1255, -0.2961, -1.0166, -0.6263,  1.3833,  0.2978,\n",
      "           1.6371],\n",
      "         [-0.9772, -0.4509, -0.9387,  0.6239,  0.4884, -1.0947, -0.6890,\n",
      "           1.5040],\n",
      "         [ 0.4192, -0.6222,  1.7720, -0.9592,  0.3621,  2.0994,  1.5402,\n",
      "           0.7701],\n",
      "         [ 3.3038, -3.1136,  2.3497, -0.8097,  1.7221, -1.0350,  1.0254,\n",
      "           0.5953]],\n",
      "\n",
      "        [[ 0.7442, -2.1441,  0.6826, -0.4069, -0.2529,  2.4642,  1.5142,\n",
      "           0.8713],\n",
      "         [-1.3663,  1.6630,  2.2280, -0.4332,  0.2386, -0.1697, -0.7749,\n",
      "           1.0554],\n",
      "         [-1.8909,  1.8972, -0.6164,  0.4871, -0.7313, -0.0037,  0.0265,\n",
      "           1.2008],\n",
      "         [ 0.3243, -2.7187,  2.7798, -0.3718,  1.7949, -1.2206,  0.0069,\n",
      "          -0.3116]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Hyperparams\n",
    "batch_size = 2\n",
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "vocab_size = 65\n",
    "\n",
    "# 2. Embedding tables\n",
    "token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "position_embedding_table = nn.Embedding(block_size, embedding_dim)\n",
    "\n",
    "# 3. Input tokens (2 sequences, 4 characters each)\n",
    "x = torch.tensor([\n",
    "    [12, 5, 8, 8],\n",
    "    [9, 1, 17, 33]\n",
    "])  # shape: (2, 4)\n",
    "\n",
    "# 4. Token embeddings (look up each token ID)\n",
    "token_emb = token_embedding_table(x)  # shape: (2, 4, 8)\n",
    "print(token_emb)\n",
    "# 5. Positional embeddings\n",
    "position_ids = torch.arange(block_size)  # [0, 1, 2, 3]\n",
    "pos_emb = position_embedding_table(position_ids)  # shape: (4, 8)\n",
    "pos_emb = pos_emb.unsqueeze(0)  # reshape to (1, 4, 8) to match batch\n",
    "\n",
    "# 6. Add them\n",
    "x_embed = token_emb + pos_emb  # shape: (2, 4, 8)\n",
    "\n",
    "print(\"Token embeddings:\\n\", token_emb)\n",
    "print(\"Positional embeddings:\\n\", pos_emb)\n",
    "print(\"Final input to transformer (x_embed):\\n\", x_embed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce21cac2-685d-4736-94b8-a8c25e8aad6a",
   "metadata": {},
   "source": [
    "# 3. Both are lookup tables\n",
    "# Token embedding table → vector for the type of token\n",
    "\n",
    "#  Positional embedding table → vector for the position of token\n",
    "\n",
    "#  Both are trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8f3f7-7d43-459c-bc35-c0cc6da2270a",
   "metadata": {},
   "source": [
    "### 🔁 Input Embedding Pipeline (with Example: \"help\")\n",
    "\n",
    "#### Input: \"help\"\n",
    "\n",
    "1. **Tokenization**  \n",
    "   - Character-level: ['h', 'e', 'l', 'p']  \n",
    "   - Token IDs (via vocab): [12, 5, 8, 9]  \n",
    "   - Shape: `(batch_size=1, block_size=4)`\n",
    "\n",
    "2. **Token Embedding**  \n",
    "   - Use `nn.Embedding(vocab_size, emb_dim)`  \n",
    "   - Maps each token ID to a learnable vector  \n",
    "   - Output shape: `(1, 4, emb_dim)`  \n",
    "   - Represents: *what* each token is\n",
    "\n",
    "3. **Positional Embedding**  \n",
    "   - Use `nn.Embedding(block_size, emb_dim)`  \n",
    "   - Creates a learnable vector for each position: [0, 1, 2, 3]  \n",
    "   - Output shape: `(1, 4, emb_dim)` (after unsqueeze)  \n",
    "   - Represents: *where* each token is in the sequence\n",
    "\n",
    "4. **Add Both Embeddings**  \n",
    "   - Final input: `token_emb + pos_emb`  \n",
    "   - Shape: `(1, 4, emb_dim)`  \n",
    "   - Each token now encodes both meaning and position\n",
    "\n",
    "✅ This combined embedding is passed to the first Transformer block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d919775-ae6c-45ce-84cc-30d99a86bf23",
   "metadata": {},
   "source": [
    "### 🔁 Project Embeddings into Query, Key, and Value (Q, K, V)\n",
    "\n",
    "To prepare for self-attention, each token's embedding is projected into 3 different vectors:\n",
    "\n",
    "- **Query (Q)** → What the token is looking for\n",
    "- **Key (K)**   → What the token offers to others\n",
    "- **Value (V)** → What the token contains to share\n",
    "\n",
    "These are created by applying 3 independent `nn.Linear` layers to the same input embedding:\n",
    "\n",
    "- Input: `x_embed` → shape `(batch_size, block_size, embedding_dim)`\n",
    "- Output:\n",
    "  - Q: `(batch_size, block_size, embedding_dim)`\n",
    "  - K: `(batch_size, block_size, embedding_dim)`\n",
    "  - V: `(batch_size, block_size, embedding_dim)`\n",
    "\n",
    "Each token is now ready to compare itself (via Q) to others (via K), and share information (via V).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0d8f7fe-4ef1-4d44-ba57-49bb7df49e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_embed shape: torch.Size([2, 4, 8])\n",
      "Q shape: torch.Size([2, 4, 8])\n",
      "K shape: torch.Size([2, 4, 8])\n",
      "V shape: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# --- Simulated inputs ---\n",
    "batch_size = 2\n",
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "\n",
    "# Simulated token+pos embeddings (e.g. from nn.Embedding)\n",
    "x_embed = torch.randn(batch_size, block_size, embedding_dim)\n",
    "\n",
    "# --- Linear layers to create Q, K, V ---\n",
    "to_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "to_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "to_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "# --- Project to Q, K, V ---\n",
    "q = to_q(x_embed)\n",
    "k = to_k(x_embed)\n",
    "v = to_v(x_embed)\n",
    "\n",
    "# --- Check shapes ---\n",
    "print(\"x_embed shape:\", x_embed.shape)  # (2, 4, 8)\n",
    "print(\"Q shape:\", q.shape)              # (2, 4, 8)\n",
    "print(\"K shape:\", k.shape)              # (2, 4, 8)\n",
    "print(\"V shape:\", v.shape)              # (2, 4, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a755acb2-2500-4056-b8de-444e56d46a16",
   "metadata": {},
   "source": [
    "# 🔁 Transformers (in the Q/K/V step)\n",
    "# You're correct — we take one input (x_embed) and pass it through three completely separate linear layers:\n",
    "\n",
    "**1.Q = Linear_Q(x_embed)**\n",
    "\n",
    "**2.K = Linear_K(x_embed)**\n",
    "\n",
    "**3.V = Linear_V(x_embed)**\n",
    "\n",
    "# These 3 layers are not connected to each other\n",
    "\n",
    "# They do not pass values between themselves\n",
    "\n",
    "# They are just three different views of the same input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640de0c3-8372-41c8-9669-6f860ccc7eab",
   "metadata": {},
   "source": [
    "### 🔍 Q/K/V Linear Layers vs Classic ANN Layers\n",
    "\n",
    "In classic neural networks (ANNs or CNNs), layers are **stacked** — the output of one layer feeds into the next in a chain:  \n",
    "`Input → Hidden → Output`.\n",
    "\n",
    "But in Transformers, during the attention step, we apply **three separate linear projections** to the same input embedding:\n",
    "\n",
    "- `Query = Linear_Q(x_embed)`\n",
    "- `Key   = Linear_K(x_embed)`\n",
    "- `Value = Linear_V(x_embed)`\n",
    "\n",
    "These are:\n",
    "- **Not connected** to each other (no flow between them)\n",
    "- **Independent** layers that each produce a different role/view of the same token\n",
    "- Essential for enabling attention:  \n",
    "  → Q compares against K to decide \"who to look at\",  \n",
    "  → V is the content actually shared.\n",
    "\n",
    "This **branching structure** is a major architectural difference from traditional neural nets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a5af6-97a7-455b-b6c1-465e4d2f1028",
   "metadata": {},
   "source": [
    "### 🎯 Step: Compute Attention Scores (Q · Kᵀ)\n",
    "\n",
    "Now that we have Query (Q) and Key (K) vectors for each token, we compute how much attention each token should pay to every other token in the sequence.\n",
    "\n",
    "- **Query**: what this token is looking for\n",
    "- **Key**: what other tokens offer\n",
    "- **Attention Score** = dot product of Q and K\n",
    "\n",
    "We calculate:\n",
    "```python\n",
    "attention_scores = Q @ Kᵀ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93338c69-7c30-4758-9ad6-432b979a5f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores shape: torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simulated Q, K (e.g., after applying nn.Linear to x_embed)\n",
    "batch_size = 2\n",
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "\n",
    "# Random Q and K for demonstration\n",
    "q = torch.randn(batch_size, block_size, embedding_dim)\n",
    "k = torch.randn(batch_size, block_size, embedding_dim)\n",
    "\n",
    "# Transpose K to prepare for dot product\n",
    "# Shape becomes (batch_size, embedding_dim, block_size)\n",
    "k_transposed = k.transpose(-2, -1)\n",
    "\n",
    "# Compute attention scores (Q @ Kᵀ)\n",
    "attention_scores = torch.matmul(q, k_transposed)\n",
    "\n",
    "print(\"Attention scores shape:\", attention_scores.shape)  # (2, 4, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52782147-24cf-4a5e-b58b-7c82bba41c44",
   "metadata": {},
   "source": [
    "### 🧠 Step: Scale and Softmax the Attention Scores\n",
    "\n",
    "After computing the raw attention scores using `Q @ Kᵀ`, we need to normalize them before using them.\n",
    "\n",
    "#### ⚠️ Why?\n",
    "- The dot products (`Q · Kᵀ`) can be large in magnitude\n",
    "- This can cause the softmax to become very sharp, leading to unstable gradients\n",
    "- We fix this by scaling the scores\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 Formula: Scaled Dot-Product Attention (before applying to V)\n",
    "\n",
    "![attention](https://media.licdn.com/dms/image/v2/D4D12AQGw6RIV4YgDOg/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1691329217886?e=2147483647&v=beta&t=LUYt7_jUybda90NoMuq1VUAFE8Gvhcdy91R2TKkHPSI)\n",
    "\n",
    "Where:\n",
    "- \\( Q \\): query matrix\n",
    "- \\( K \\): key matrix\n",
    "- \\( d_k \\): dimension of the key/query vectors\n",
    "- Softmax is applied along each token's row\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ What we achieve:\n",
    "- All attention scores become **probabilities** (sum to 1 for each token)\n",
    "- Each token now has a meaningful **attention distribution** over others\n",
    "- These weights will be used to combine the **value (V)** vectors in the next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "062ed285-17f1-4e9b-9aea-5e5e49dddbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: torch.Size([2, 4, 4])\n",
      "Row sums for first batch item:\n",
      " tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Assume we already have:\n",
    "# attention_scores from Q @ Kᵀ\n",
    "# and embedding_dim from earlier\n",
    "\n",
    "# Example shape: (batch_size=2, block_size=4, block_size=4)\n",
    "# Each token attends to every other token in its sequence\n",
    "\n",
    "# Scale the attention scores\n",
    "scaled_scores = attention_scores / math.sqrt(embedding_dim)\n",
    "\n",
    "# Apply softmax to get attention weights (probabilities)\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "# Print shape and check row sums (should be ~1)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)\n",
    "print(\"Row sums for first batch item:\\n\", attention_weights[0].sum(dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c707ec8f-0033-4392-95a2-14db9546d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# V from earlier: shape (batch_size, block_size, embedding_dim)\n",
    "# attention_weights: shape (batch_size, block_size, block_size)\n",
    "\n",
    "# Multiply attention weights with V to get final attention output\n",
    "attention_output = torch.matmul(attention_weights, v)\n",
    "\n",
    "print(\"Attention output shape:\", attention_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8e516-fb07-4cda-a4ab-e3afa482e4c5",
   "metadata": {},
   "source": [
    "# ✅ What we’ve built so far:\n",
    "# A complete forward pass of self-attention (1 head)\n",
    "\n",
    "# With deep understanding of:\n",
    "\n",
    "# What Q/K/V really mean\n",
    "\n",
    "# How tokens interact\n",
    "\n",
    "# How info flows in attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ea4b2-3ccc-4b04-9b3c-f30f3a0ef6ee",
   "metadata": {},
   "source": [
    "### ✅ Summary So Far: Self-Attention (Single Head)\n",
    "\n",
    "1. **Tokenization**: Converted characters into integer token IDs.\n",
    "2. **Embeddings**: Used `nn.Embedding` to create:\n",
    "   - Token embeddings (meaning of token)\n",
    "   - Position embeddings (position in sequence)\n",
    "   - Final input: `x_embed = token_embed + pos_embed`\n",
    "\n",
    "3. **Q, K, V Projection**:\n",
    "   - 3 `nn.Linear` layers applied to `x_embed`\n",
    "   - Each token gets: Query (Q), Key (K), Value (V)\n",
    "\n",
    "4. **Attention Scores**:\n",
    "   - Computed: `scores = Q @ Kᵀ`\n",
    "   - Result: how much each token attends to others\n",
    "\n",
    "5. **Softmax Attention Weights**:\n",
    "   - Scaled by `sqrt(embedding_dim)`\n",
    "   - Applied softmax → get attention weights (probabilities)\n",
    "\n",
    "6. **Weighted Sum with V**:\n",
    "   - Used weights to combine V: `output = attention_weights @ V`\n",
    "   - Final result: context-aware embedding per token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59a62a-4f7f-4dcc-9bc7-66041cc3f9b8",
   "metadata": {},
   "source": [
    "### 🧠 Transformer Block: Neural Network Architecture (So Far)\n",
    "\n",
    "We’ve implemented the self-attention mechanism as a neural network.\n",
    "\n",
    "#### 🔷 Step-by-step Flow:\n",
    "\n",
    "1. **Input Tokens**\n",
    "   - Example: \"help\" → token IDs → `[12, 5, 8, 9]`\n",
    "\n",
    "2. **Embedding Layer**\n",
    "   - `token_embedding = nn.Embedding(vocab_size, emb_dim)`\n",
    "   - `position_embedding = nn.Embedding(block_size, emb_dim)`\n",
    "   - Final embedding:  \n",
    "     \\[\n",
    "     x_{\\text{embed}} = \\text{token\\_embedding}(x) + \\text{position\\_embedding}(pos)\n",
    "     \\]\n",
    "\n",
    "3. **Parallel Projections: Q, K, V**\n",
    "   - 3 separate `nn.Linear` layers:\n",
    "     ```python\n",
    "     Q = Linear_Q(x_embed)\n",
    "     K = Linear_K(x_embed)\n",
    "     V = Linear_V(x_embed)\n",
    "     ```\n",
    "   - Each token is now represented as:\n",
    "     - Query → what it wants to look for\n",
    "     - Key   → what it offers\n",
    "     - Value → what it contains\n",
    "\n",
    "4. **Self-Attention Calculation (not a layer, just math)**\n",
    "   - Compute scores: `scores = Q @ Kᵀ`\n",
    "   - Scale: `scaled = scores / sqrt(embedding_dim)`\n",
    "   - Normalize: `weights = softmax(scaled, dim=-1)`\n",
    "   - Weighted sum: `attention_output = weights @ V`\n",
    "\n",
    "#### ✅ Key Idea:\n",
    "- Q, K, V are computed **in parallel** from `x_embed`\n",
    "- Self-attention is computed **after Q, K, V are ready**\n",
    "- Attention is a dynamic, learned way of combining token information\n",
    "\n",
    "Each token's output is now a **context-aware vector** that \"knows about\" other tokens in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87af5a-2976-46fa-81c6-62056e1c5009",
   "metadata": {},
   "source": [
    "### 🧠 Final Linear Projection after Attention\r\n",
    "\r\n",
    "After computing the context-aware vectors from attention (`attention_weights @ V`), we apply a final `nn.Linear` layer:\r\n",
    "\r\n",
    "$$\r\n",
    "\\text{output} = W_o \\cdot (\\text{attention\\_output}) + b_o\r\n",
    "$$\r\n",
    "\r\n",
    "#### ✅ Why this step?\r\n",
    "- The attention step itself uses no learnable parameters beyond Q/K/V\r\n",
    "- This linear layer introduces **new trainable weights**\r\n",
    "- It helps the model **transform** and **refine** the attended information\r\n",
    "- Keeps output shape the same: `(batch_size, block_size, embedding_dim)`\r\n",
    "\r\n",
    "This is the last part of the **self-attention block** (before things like residuals or MLP).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f16f7d8-6f8c-44b0-8778-0ccd4ae708fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after final projection: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Let's assume this is the attention output from earlier\n",
    "# Shape: (batch_size, block_size, embedding_dim)\n",
    "attention_output = torch.randn(2, 4, 8)  # Example shape\n",
    "\n",
    "# Final linear projection layer (learnable)\n",
    "out_proj = nn.Linear(8, 8)  # input and output dim = embedding_dim\n",
    "\n",
    "# Apply the projection\n",
    "output = out_proj(attention_output)\n",
    "\n",
    "print(\"Output shape after final projection:\", output.shape)  # (2, 4, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbab619-dd1d-463f-8601-22dea6d03505",
   "metadata": {},
   "source": [
    "### 🔁 Step: Residual Connection (Post-Attention)\n",
    "\n",
    "After computing the attention output and passing it through a linear layer, we apply a **residual (skip) connection** by adding the original input back to the output.\n",
    "\n",
    "#### 🔹 Why?\n",
    "- Helps preserve the original input (`x_embed`)\n",
    "- Prevents loss of information\n",
    "- Makes training more stable and enables deeper models\n",
    "- Allows gradients to flow more easily through the network\n",
    "\n",
    "#### 🔹 Operation:\n",
    "If `x_embed` is the original input and `attn_out` is the projected attention output:\n",
    "\n",
    "```python\n",
    "x = x_embed + attn_out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f3b28-09fd-4378-ab75-5588e5cd7f59",
   "metadata": {},
   "source": [
    "### 🧪 Layer Normalization Summary\n",
    "\n",
    "Layer Normalization (`nn.LayerNorm`) is applied after the residual connection in transformer blocks.\n",
    "\n",
    "#### 🔹 Purpose:\n",
    "- Stabilizes training\n",
    "- Normalizes activations across the **embedding dimension**\n",
    "- Keeps gradients well-behaved and prevents exploding/vanishing values\n",
    "\n",
    "#### 🔹 How it works:\n",
    "For each token vector `x_i` of shape `(embedding_dim)`, it normalizes as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x_i) = \\frac{x_i - \\mu}{\\sigma} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean of the features\n",
    "- $\\sigma$ = standard deviation of the features\n",
    "- $\\gamma$ and $\\beta$ are learnable scale and shift parameters\n",
    "\n",
    "#### 🔹 In Code:\n",
    "```python\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "x = layer_norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51b574df-cdaa-4913-81d7-993b2a10e045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape after residual + layer norm: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Simulated input and attention output ---\n",
    "batch_size = 2\n",
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "\n",
    "# Original input to attention block (from embeddings)\n",
    "x_embed = torch.randn(batch_size, block_size, embedding_dim)\n",
    "\n",
    "# Attention output after projection (from previous steps)\n",
    "attn_out = torch.randn(batch_size, block_size, embedding_dim)\n",
    "\n",
    "# --- Residual connection ---\n",
    "x = x_embed + attn_out  # Add original input (residual connection)\n",
    "\n",
    "# --- Layer Normalization ---\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "x = layer_norm(x)       # Normalize across embedding dimension\n",
    "\n",
    "print(\"Final shape after residual + layer norm:\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df39c7-5d45-4585-8ee2-ab1957a6bc98",
   "metadata": {},
   "source": [
    "### 🔧 Feedforward Block (MLP)\n",
    "\n",
    "After attention, each token embedding is passed through a small neural network called the **Feedforward Block** or **MLP block**.\n",
    "\n",
    "#### 🔹 Purpose:\n",
    "- Applies a **non-linear transformation** to each token's vector\n",
    "- Helps the model learn deeper representations\n",
    "- Complements attention (which focuses on context) with local computation\n",
    "\n",
    "#### 🔹 Architecture (Per Token):\n",
    "1. Linear layer: expands the vector (typically 4× embedding size)\n",
    "2. GELU activation: adds non-linearity\n",
    "3. Linear layer: projects back to original embedding size\n",
    "\n",
    "#### 🔹 Formula:\n",
    "Let `x` be the token vector of shape `(embedding_dim)`:\n",
    "\n",
    "```python\n",
    "x = Linear_2(GELU(Linear_1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3eecbee-d39e-4777-8734-8e1b518c6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward output shape: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Setup ---\n",
    "batch_size = 2\n",
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "hidden_dim = 4 * embedding_dim  # Typically 4x for transformers\n",
    "\n",
    "# Simulated input from previous LayerNorm (shape: B, T, C)\n",
    "x = torch.randn(batch_size, block_size, embedding_dim)\n",
    "\n",
    "# --- Feedforward Block ---\n",
    "ffn = nn.Sequential(\n",
    "    nn.Linear(embedding_dim, hidden_dim),  # Expand\n",
    "    nn.GELU(),                              # Non-linearity\n",
    "    nn.Linear(hidden_dim, embedding_dim)    # Project back\n",
    ")\n",
    "\n",
    "# Apply feedforward block\n",
    "ffn_output = ffn(x)\n",
    "\n",
    "print(\"Feedforward output shape:\", ffn_output.shape)  # (B, T, C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e87de-aea9-4c28-abb2-53ab23e1b373",
   "metadata": {},
   "source": [
    "Here’s your clean **notebook markdown summary** for the **residual + layer norm after the MLP block** — ready to paste:\n",
    "\n",
    "````markdown\n",
    "### 🔁 Residual + LayerNorm (Post-Feedforward)\n",
    "\n",
    "After the feedforward (MLP) block, we apply:\n",
    "\n",
    "1. **Residual Connection**\n",
    "   - Add the MLP output to its input:\n",
    "   ```python\n",
    "   x = x + mlp_out\n",
    "````\n",
    "\n",
    "* This helps preserve the original signal and stabilize gradients.\n",
    "\n",
    "2. **Layer Normalization**\n",
    "\n",
    "   * Normalizes the combined output:\n",
    "\n",
    "   ```python\n",
    "   x = LayerNorm(x)\n",
    "   ```\n",
    "\n",
    "   * Ensures consistent activation scales across tokens.\n",
    "\n",
    "#### 🔹 Why this matters:\n",
    "\n",
    "* Helps the network train deeper without losing information\n",
    "* Maintains stable learning and smooth flow of gradients\n",
    "* Standard in every transformer block\n",
    "\n",
    "This completes the **full transformer block**:\n",
    "\n",
    "* Attention → residual + norm\n",
    "* MLP → residual + norm ✅\n",
    "\n",
    "```\n",
    "\n",
    "Let me know if you’re ready to move on to:\n",
    "> 🔄 Stacking multiple transformer blocks — or building the final output head!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "496c22af-dba4-459e-ab44-bfdadb9b3765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output after MLP + residual + norm: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Setup ---\n",
    "batch_size = 2\n",
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "hidden_dim = 4 * embedding_dim\n",
    "\n",
    "# Input to feedforward (output of previous layernorm)\n",
    "x = torch.randn(batch_size, block_size, embedding_dim)\n",
    "\n",
    "# Feedforward block\n",
    "ffn = nn.Sequential(\n",
    "    nn.Linear(embedding_dim, hidden_dim),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(hidden_dim, embedding_dim)\n",
    ")\n",
    "\n",
    "# Apply MLP\n",
    "mlp_out = ffn(x)\n",
    "\n",
    "# Residual connection\n",
    "x = x + mlp_out\n",
    "\n",
    "# LayerNorm\n",
    "mlp_norm = nn.LayerNorm(embedding_dim)\n",
    "x = mlp_norm(x)\n",
    "\n",
    "print(\"Final output after MLP + residual + norm:\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582d46d-c571-4436-98f3-7e384d5e753e",
   "metadata": {},
   "source": [
    "````markdown\n",
    "## ✅ Transformer Decoder Block: Full Summary (So Far)\n",
    "\n",
    "We’ve built a **full GPT-style transformer decoder block** step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 1. Load and Tokenize Dataset\n",
    "- Loaded TinyShakespeare text (character-level)\n",
    "- Tokenized text into integer IDs\n",
    "- Created `(x, y)` input-output pairs for **next token prediction**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Embeddings\n",
    "- Used `nn.Embedding` to create:\n",
    "  - **Token Embeddings** → convert token IDs into dense vectors\n",
    "  - **Positional Embeddings** → inject token position info\n",
    "- Added them:\n",
    "  ```python\n",
    "  x_embed = token_embedding + position_embedding\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. Self-Attention Block\n",
    "\n",
    "* Projected `x_embed` to Query, Key, Value:\n",
    "\n",
    "  ```python\n",
    "  Q = Linear(x_embed), K = Linear(x_embed), V = Linear(x_embed)\n",
    "  ```\n",
    "* Computed attention scores:\n",
    "\n",
    "  ```python\n",
    "  weights = softmax(Q @ Kᵀ / sqrt(d_k))\n",
    "  ```\n",
    "* Combined values:\n",
    "\n",
    "  ```python\n",
    "  attn_output = weights @ V\n",
    "  ```\n",
    "* Applied a final linear projection\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 4. Residual + LayerNorm (After Attention)\n",
    "\n",
    "* Skip connection:\n",
    "\n",
    "  ```python\n",
    "  x = x_embed + attn_output\n",
    "  ```\n",
    "* Normalize:\n",
    "\n",
    "  ```python\n",
    "  x = LayerNorm(x)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 5. Feedforward (MLP Block)\n",
    "\n",
    "* Applied to each token vector:\n",
    "\n",
    "  ```python\n",
    "  x = Linear → GELU → Linear\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 6. Residual + LayerNorm (After MLP)\n",
    "\n",
    "* Added residual:\n",
    "\n",
    "  ```python\n",
    "  x = x + mlp_output\n",
    "  ```\n",
    "* Applied layer norm:\n",
    "\n",
    "  ```python\n",
    "  x = LayerNorm(x)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ At this point:\n",
    "\n",
    "You’ve implemented:\n",
    "\n",
    "* 🧱 The full Transformer **decoder block**\n",
    "* 📏 Output shape: `(batch_size, block_size, embedding_dim)`\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44ab28-61a8-4594-b011-815b1e80cf0a",
   "metadata": {},
   "source": [
    "Awesome! Here's the **notebook markdown summary** for the **final output layer** — Linear → Softmax:\n",
    "\n",
    "---\n",
    "\n",
    "````markdown\n",
    "### 🎯 Final Output Layer (Linear → Softmax)\n",
    "\n",
    "Once we get the output from the transformer block, we map each token vector to vocab scores.\n",
    "\n",
    "#### 🔹 1. Linear Projection\n",
    "Each token vector has shape `(embedding_dim)`, but we want a score for **each token in the vocabulary**.\n",
    "\n",
    "We use a final `Linear(embedding_dim, vocab_size)`:\n",
    "```python\n",
    "logits = final_linear(x)\n",
    "````\n",
    "\n",
    "* Shape: `(batch_size, block_size, vocab_size)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 2. Softmax (optional during inference)\n",
    "\n",
    "To convert logits to probabilities (only needed during inference):\n",
    "\n",
    "```python\n",
    "probs = softmax(logits, dim=-1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ❗ Training Note:\n",
    "\n",
    "* During training, we use `nn.CrossEntropyLoss`\n",
    "* This **does not require softmax**, because it's applied internally\n",
    "\n",
    "```python\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits.view(-1, vocab_size), targets.view(-1))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ This completes the transformer pass!\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7eda231-e084-44fc-b11b-7346c0e1665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 4, 65])\n",
      "Loss: 4.267731189727783\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Setup ---\n",
    "batch_size = 2\n",
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "vocab_size = 65  # for TinyShakespeare (example)\n",
    "\n",
    "# Simulated transformer output (after MLP + LayerNorm)\n",
    "x = torch.randn(batch_size, block_size, embedding_dim)\n",
    "\n",
    "# Final output projection layer\n",
    "final_linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "# Get logits (scores for each token in the vocab)\n",
    "logits = final_linear(x)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)  # (B, T, vocab_size)\n",
    "\n",
    "# Example: Compute loss with ground-truth target tokens\n",
    "targets = torch.randint(0, vocab_size, (batch_size, block_size))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f2af7-feed-4cd0-88cd-1f29dd5342e4",
   "metadata": {},
   "source": [
    "# Full forward pass single head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebfe72bd-4c00-4367-9a44-fd6db4d57614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 4.3123\n",
      "Step 50: Loss = 2.9114\n",
      "Step 100: Loss = 2.7385\n",
      "Step 150: Loss = 2.6023\n",
      "Step 200: Loss = 2.5554\n",
      "Step 250: Loss = 2.4493\n",
      "Step 300: Loss = 2.4366\n",
      "Step 350: Loss = 2.4248\n",
      "Step 400: Loss = 2.3470\n",
      "Step 450: Loss = 2.2953\n",
      "HABSANIULF:\n",
      "MArdis st withy natrece!\n",
      "QUBrowastory fou, me soall, wiand swee ar must: toss fand courd frit,\n",
      "LAn's?\n",
      "\n",
      "\n",
      "MamSiven thanvers tase indy's Fid:\n",
      "And youre agenads Sweing hes poweyce thy out handa\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# --- Load dataset ---\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Mapping\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for ch,i in stoi.items() }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "train_data = data[:int(0.9*len(data))]\n",
    "val_data = data[int(0.9*len(data)):]\n",
    "\n",
    "# --- Batch preparation ---\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# --- GPT Model ---\n",
    "class MiniGPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embed = nn.Embedding(block_size, embedding_dim)\n",
    "\n",
    "        self.to_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.to_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.to_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.attn_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.attn_layernorm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim)\n",
    "        )\n",
    "        self.ff_layernorm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embed(idx)\n",
    "        pos_ids = torch.arange(T, device=idx.device)\n",
    "        pos_emb = self.pos_embed(pos_ids)[None, :, :]\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (q.shape[-1] ** 0.5)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            torch.triu(torch.ones(T, T, device=idx.device), 1).bool(),\n",
    "            float('-inf')\n",
    "        )\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = attn_weights @ v\n",
    "        attn_output = self.attn_proj(attn_output)\n",
    "\n",
    "        x = self.attn_layernorm(x + attn_output)\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.ff_layernorm(x + ff_out)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx\n",
    "\n",
    "# --- Hyperparams and Training ---\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embedding_dim = 64\n",
    "block_size = 64\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "max_iters = 500\n",
    "\n",
    "model = MiniGPTModel(vocab_size, block_size, embedding_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iters):\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# --- Sample from the model ---\n",
    "context = torch.tensor([[stoi[\"H\"]]], dtype=torch.long).to(device)\n",
    "generated = model.generate(context, max_new_tokens=200)[0].tolist()\n",
    "print(decode(generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b5f2e98-822c-4199-9c5c-7af3f02e9597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\n",
      "LI this sto gof ary,\n",
      "We farcheesomak, ges deseembrit thert thanelvenewveo,\n",
      "Go,\n",
      "DUind;\n",
      "CHaserbyneld dy wicw galmars: pim nin eris oventle se?\n",
      "\n",
      "\n",
      "JYy far arin h; no sfupe so'st sot.\n",
      "\n",
      "Amis anesh\n",
      "Prbewfo\n"
     ]
    }
   ],
   "source": [
    "# --- Sample from the model ---\n",
    "context = torch.tensor([[stoi[\"I\"]]], dtype=torch.long).to(device)\n",
    "generated = model.generate(context, max_new_tokens=200)[0].tolist()\n",
    "print(decode(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b0614-68b9-4cbd-9339-0d61eeac0477",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9442ab5-c720-4ecb-b4f9-111880fc74a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Loss: 4.3203\n",
      "Step 100 | Loss: 2.4496\n",
      "Step 200 | Loss: 2.2034\n",
      "Step 300 | Loss: 1.9375\n",
      "Step 400 | Loss: 1.7223\n",
      "Step 500 | Loss: 1.5520\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "train_data = data[:int(0.9*len(data))]\n",
    "val_data = data[int(0.9*len(data)):]\n",
    "\n",
    "# Batching\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, block_size):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.qkv_proj = nn.Linear(embedding_dim, 3 * embedding_dim)\n",
    "        self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_proj(x).view(B, T, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), 1).bool()\n",
    "        attn_scores = attn_scores.masked_fill(mask, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = attn_weights @ v\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, block_size):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(embedding_dim, num_heads, block_size)\n",
    "        self.attn_ln = nn.LayerNorm(embedding_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim)\n",
    "        )\n",
    "        self.ff_ln = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn_ln(x + self.attn(x))\n",
    "        x = self.ff_ln(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "# GPT Model\n",
    "class MiniGPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embed = nn.Embedding(block_size, embedding_dim)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embedding_dim, num_heads, block_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embed(idx)\n",
    "        pos_ids = torch.arange(T, device=idx.device)\n",
    "        pos_emb = self.pos_embed(pos_ids)[None, :, :]\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx\n",
    "\n",
    "# Training Setup (Bigger & Deeper)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embedding_dim = 512\n",
    "block_size = 256\n",
    "batch_size = 128\n",
    "learning_rate = 3e-4\n",
    "max_iters = 3000\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "\n",
    "model = MiniGPTModel(vocab_size, block_size, embedding_dim, num_heads, num_layers).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for step in range(max_iters):\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Sampling Output\n",
    "context = torch.tensor([[stoi[\"H\"]]], dtype=torch.long).to(device)\n",
    "generated = model.generate(context, max_new_tokens=300)[0].tolist()\n",
    "print(decode(generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26d01253-0bb0-4e5e-9470-a39ef0d053f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hongen,\n",
      "Mur wheresere goitles your kined notur'd!\n",
      "Whe mirshene tuth duch no dexdees, you beake goon\n",
      "A butings.\n",
      "\n",
      "Past mongited ase:\n",
      "Nowel win that sim crakst.\n",
      "\n",
      "VENG YORETE:\n",
      "Haste bealfe to geare, is hon lathend theieang\n",
      "I modscam bett thinker meathts auls woulf\n",
      "Mirve thes pa his we inde ream mofecord \n"
     ]
    }
   ],
   "source": [
    "# Sampling\n",
    "context = torch.tensor([[stoi[\"H\"]]], dtype=torch.long).to(device)\n",
    "generated = model.generate(context, max_new_tokens=300)[0].tolist()\n",
    "print(decode(generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
