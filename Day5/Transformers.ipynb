{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd73feb1-7ba8-4206-9796-0aca06fe2a91",
   "metadata": {},
   "source": [
    "### 🧠 Decoder‑Only Transformer Architecture\n",
    "\n",
    "![Decoder‑Only Transformer Diagram](https://waylandzhang.github.io/en/images/decoder-only-transformer.jpg)\n",
    "\n",
    "**Figure:** A GPT‑style stack of decoder blocks:\n",
    "- **Input tokens** → Embedding + positional encoding\n",
    "- **Repeated blocks**: masked self-attention + feed‑forward + layer norms + residuals\n",
    "- **Final linear & softmax** → predict next-token logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb7d14-05a8-4ea6-903b-ade6067a2347",
   "metadata": {},
   "source": [
    "# Load the Dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a85b081-a8f9-4cf1-a953-057b767a0b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1115394\n",
      "First 500 characters:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "print(\"Dataset length:\", len(text))\n",
    "print(\"First 500 characters:\\n\", text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203abed-1e84-4dcf-ab75-961e519d3c49",
   "metadata": {},
   "source": [
    "# Bulid Vocab\n",
    "# 🧠 What is Tokenization?\n",
    "# Tokenization is the process of converting text into units (tokens) that a neural network can understand — and then mapping those tokens to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fed924-1776-47df-857d-19ab0cf38f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902617d6-4f6a-4d95-99ad-e5cc4c527039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f12f65c-3b9c-45fc-898b-077f7a1e0193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer dictionaries\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "dict(list(stoi.items())[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1ebce4-11e5-429d-83cd-570bea036125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = {i: ch for ch, i in stoi.items()}  \n",
    "dict(list(itos.items())[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5bbce-c6e6-4971-ac85-052d474f1c4c",
   "metadata": {},
   "source": [
    "#  ✅ In PyTorch, the first real step before model training is:\n",
    "# 🔁 Convert all input data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfb53d6-5a2c-483a-84b3-8a1c0b0109de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset shape: torch.Size([1115394])\n",
      "First 20 tokens: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56])\n"
     ]
    }
   ],
   "source": [
    "# Step 1.2 — Encode entire dataset\n",
    "import torch\n",
    "\n",
    "# Convert the full text to a list of token IDs\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
    "\n",
    "print(\"Tokenized dataset shape:\", data.shape)\n",
    "print(\"First 20 tokens:\", data[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184575e4-a615-4a2e-80b1-9caa48f13abf",
   "metadata": {},
   "source": [
    "# Split the data into train and val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3dc5ff-e97f-4cd9-8545-5e85bf0270d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1003854\n",
      "Val size: 111540\n"
     ]
    }
   ],
   "source": [
    "# Step 1.3 — Split data into train and val\n",
    "split_idx = int(0.9 * len(data))  # 90% train, 10% val\n",
    "\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Val size:\", len(val_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf3e0a0-362e-4390-b70d-6e6f4ae6d34b",
   "metadata": {},
   "source": [
    "# Create Training Batches (x, y pairs)\n",
    "# 🧱 What is batch_size?\n",
    "# batch_size is the number of (x, y) training examples processed in one forward/backward pass of the model.\n",
    "# ⛓ Why batch?\n",
    "# Matrix operations (on GPU) are fastest when done in batches\n",
    "\n",
    "# You get more stable gradients\n",
    "\n",
    "# You reduce variance compared to updating on just one example -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4021b2-40ad-46a2-97fa-f296a19bb813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, block_size=8, batch_size=4):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb11c25-6e21-4c9c-88a9-a202d2d0326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[58, 11,  0, 20],\n",
      "        [43,  6,  1, 61]])\n",
      "y:\n",
      " tensor([[11,  0, 20, 43],\n",
      "        [ 6,  1, 61, 47]])\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data, block_size=4, batch_size=2)\n",
    "\n",
    "print(\"x:\\n\", x)\n",
    "print(\"y:\\n\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c283c15-4708-4850-beeb-0d4c6c821d6b",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3397e-4e97-4ede-9308-db6ac7bb4ce3",
   "metadata": {},
   "source": [
    "# 🔹 What exactly is nn.Embedding in PyTorch?\n",
    "# 🧠 High-level idea\n",
    "# nn.Embedding is just a lookup table:\n",
    "# It stores one vector (a list of numbers) for each token ID\n",
    "# 🧱 Summary:\n",
    "# Concept\tExplanation\n",
    "# What it is\tA learnable matrix of shape (vocab_size, emb_dim)\n",
    "# What it does\tLooks up vectors for input token IDs\n",
    "# Is it a neural net layer?\t✅ Yes (has weights, supports backprop)\n",
    "# Why we use it\tTo represent tokens as meaningful vectors\n",
    "\n",
    "# 🧠 So when does it become trainable?\n",
    "# Here’s the secret:\n",
    "\n",
    "# This embedding matrix is a parameter (like any layer's weights)\n",
    "\n",
    "# During training, we calculate loss, then do .backward()\n",
    "\n",
    "# PyTorch computes gradients w.r.t. the rows used (e.g., token 12, 5, 8)\n",
    "\n",
    "# Only those rows get updated in .step()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f41f178-c482-4f8d-bf9c-f0e18727854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs shape: torch.Size([1, 4])\n",
      "Embedded vector shape: torch.Size([1, 4, 32])\n",
      "Embedded vectors:\n",
      " tensor([[[ 1.6122, -0.0042,  3.2233, -0.8774, -0.8261, -0.6892, -0.0900,\n",
      "          -0.3842, -0.5861, -0.3335, -1.0340, -2.3457, -0.6595,  0.5727,\n",
      "           0.6329,  0.7235,  0.2713,  0.1399, -1.1759,  1.0062,  0.7798,\n",
      "          -1.5368,  0.1864,  0.3425, -1.0717,  1.2803,  2.6648,  1.6649,\n",
      "          -1.4183,  0.4762,  0.4356,  1.9057],\n",
      "         [ 1.9596,  1.0546,  0.0424,  0.4907, -0.5999, -0.6488,  0.2887,\n",
      "          -1.2818,  1.1660,  0.5406, -0.7223,  0.4687, -0.3591, -0.2321,\n",
      "          -0.7401,  0.4454, -1.2618, -0.3058,  0.6854, -0.7882,  0.4058,\n",
      "           0.5952,  1.1985,  0.7040, -1.4199, -1.1844,  0.9307,  0.6872,\n",
      "          -0.1857,  1.6204,  0.9634,  1.4590],\n",
      "         [-1.5320, -1.3053, -1.5015,  0.7131,  1.0173, -0.7714,  0.3690,\n",
      "           0.0819, -1.4902, -1.5129, -1.9582, -0.0289, -0.0393,  0.4617,\n",
      "           0.1914,  1.0228, -0.0082,  2.5797, -0.1569,  1.4555, -1.7547,\n",
      "          -0.0446, -0.0442,  0.4607,  0.5822,  1.0772, -0.7683, -0.0051,\n",
      "          -1.6814, -0.0657, -1.5033, -0.7830],\n",
      "         [-1.5320, -1.3053, -1.5015,  0.7131,  1.0173, -0.7714,  0.3690,\n",
      "           0.0819, -1.4902, -1.5129, -1.9582, -0.0289, -0.0393,  0.4617,\n",
      "           0.1914,  1.0228, -0.0082,  2.5797, -0.1569,  1.4555, -1.7547,\n",
      "          -0.0446, -0.0442,  0.4607,  0.5822,  1.0772, -0.7683, -0.0051,\n",
      "          -1.6814, -0.0657, -1.5033, -0.7830]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Vocabulary size — number of unique tokens\n",
    "vocab_size = 65\n",
    "\n",
    "# Embedding dimension — how big each token's vector should be\n",
    "embedding_dim = 32\n",
    "\n",
    "# Step 1: Create the embedding layer\n",
    "token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Step 2: Create a batch of token IDs\n",
    "x = torch.tensor([[12, 5, 8, 8]])  # shape = (batch_size=1, block_size=4)\n",
    "\n",
    "# Step 3: Apply the embedding layer\n",
    "x_embed = token_embedding_table(x)\n",
    "\n",
    "# Step 4: Print shapes and values\n",
    "print(\"Input token IDs shape:\", x.shape)\n",
    "print(\"Embedded vector shape:\", x_embed.shape)\n",
    "print(\"Embedded vectors:\\n\", x_embed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73386dbc-232c-4e29-b3aa-3e034dcff8c9",
   "metadata": {},
   "source": [
    "# Add Positional Encoding\n",
    "# 🧠 Why?\n",
    "# Transformers have no sense of order — they treat all tokens as a bag of vectors.\n",
    "\n",
    "# But language is ordered:\n",
    "\n",
    "# \"The cat sat\" ≠ \"Sat the cat\"\n",
    "\n",
    "# So we must inject position information into each token’s embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9012ec06-e8e4-4036-a01d-a6ce36ccf4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of table: torch.Size([4, 8])\n",
      "Positional Embedding Table:\n",
      " Parameter containing:\n",
      "tensor([[ 0.1188,  0.5387,  3.2176, -1.8982, -0.6270, -1.3672,  0.8717, -0.5323],\n",
      "        [ 1.1725, -1.8441,  0.8879, -0.3920, -0.8380, -0.4821, -0.8837,  1.1680],\n",
      "        [-1.3120, -0.4717,  1.0688, -0.5945,  0.4471, -1.8819,  0.8677, -1.4532],\n",
      "        [-2.1060, -0.9682, -0.2744, -1.1484,  0.1869,  0.8414, -0.6437, -0.0129]],\n",
      "       requires_grad=True)\n",
      "Vector for position 2:\n",
      " tensor([[-1.3120, -0.4717,  1.0688, -0.5945,  0.4471, -1.8819,  0.8677, -1.4532]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Position vectors:\n",
      " tensor([[ 0.1188,  0.5387,  3.2176, -1.8982, -0.6270, -1.3672,  0.8717, -0.5323],\n",
      "        [ 1.1725, -1.8441,  0.8879, -0.3920, -0.8380, -0.4821, -0.8837,  1.1680],\n",
      "        [-1.3120, -0.4717,  1.0688, -0.5945,  0.4471, -1.8819,  0.8677, -1.4532],\n",
      "        [-2.1060, -0.9682, -0.2744, -1.1484,  0.1869,  0.8414, -0.6437, -0.0129]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "position_embedding_table = nn.Embedding(block_size, embedding_dim)\n",
    "print(\"Shape of table:\", position_embedding_table.weight.shape)\n",
    "print(\"Positional Embedding Table:\\n\", position_embedding_table.weight)\n",
    "pos_vector = position_embedding_table(torch.tensor([2]))\n",
    "print(\"Vector for position 2:\\n\", pos_vector)\n",
    "position_ids = torch.arange(block_size)\n",
    "pos_vectors = position_embedding_table(position_ids)\n",
    "print(\"Position vectors:\\n\", pos_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64da19f1-c04e-4db2-871c-d8d188a13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8800, -0.3574, -1.5421, -0.0831,  1.5632, -1.4468,  0.7656,\n",
      "          -1.0957],\n",
      "         [ 0.0065,  0.1111,  0.4171, -0.7003,  0.5837, -0.8860, -1.5983,\n",
      "          -0.6776],\n",
      "         [ 1.8465, -0.4211, -0.0725, -0.2611, -0.1395,  0.5778,  0.4730,\n",
      "          -0.0669],\n",
      "         [ 1.8465, -0.4211, -0.0725, -0.2611, -0.1395,  0.5778,  0.4730,\n",
      "          -0.0669]],\n",
      "\n",
      "        [[-0.8325, -1.5855, -0.7591, -1.0580,  0.1029, -0.6641, -0.0900,\n",
      "           0.3601],\n",
      "         [-0.2799, -0.7474,  0.0538, -0.3732,  0.1258,  0.1530,  0.9843,\n",
      "          -1.2262],\n",
      "         [-1.1766, -0.0931,  0.3913,  0.6146,  0.3958,  0.6541, -0.5164,\n",
      "           0.6811],\n",
      "         [-0.9442,  0.8399, -1.3139, -0.0559,  0.3236, -0.8218, -1.2157,\n",
      "           0.8498]]], grad_fn=<EmbeddingBackward0>)\n",
      "Token embeddings:\n",
      " tensor([[[ 1.8800, -0.3574, -1.5421, -0.0831,  1.5632, -1.4468,  0.7656,\n",
      "          -1.0957],\n",
      "         [ 0.0065,  0.1111,  0.4171, -0.7003,  0.5837, -0.8860, -1.5983,\n",
      "          -0.6776],\n",
      "         [ 1.8465, -0.4211, -0.0725, -0.2611, -0.1395,  0.5778,  0.4730,\n",
      "          -0.0669],\n",
      "         [ 1.8465, -0.4211, -0.0725, -0.2611, -0.1395,  0.5778,  0.4730,\n",
      "          -0.0669]],\n",
      "\n",
      "        [[-0.8325, -1.5855, -0.7591, -1.0580,  0.1029, -0.6641, -0.0900,\n",
      "           0.3601],\n",
      "         [-0.2799, -0.7474,  0.0538, -0.3732,  0.1258,  0.1530,  0.9843,\n",
      "          -1.2262],\n",
      "         [-1.1766, -0.0931,  0.3913,  0.6146,  0.3958,  0.6541, -0.5164,\n",
      "           0.6811],\n",
      "         [-0.9442,  0.8399, -1.3139, -0.0559,  0.3236, -0.8218, -1.2157,\n",
      "           0.8498]]], grad_fn=<EmbeddingBackward0>)\n",
      "Positional embeddings:\n",
      " tensor([[[ 0.2150, -0.2183,  1.4908,  0.0473, -0.2598, -0.9400, -0.2672,\n",
      "          -1.0819],\n",
      "         [-0.6514,  0.7042,  0.2814, -0.6853,  0.3943,  0.3365,  1.6037,\n",
      "          -0.2821],\n",
      "         [ 1.8895, -0.2058,  0.7825,  0.9717,  1.5825,  1.3741,  0.3480,\n",
      "           0.1990],\n",
      "         [-1.5429,  0.8137, -0.8867,  0.5593, -0.8957,  0.2131,  1.3705,\n",
      "           0.8867]]], grad_fn=<UnsqueezeBackward0>)\n",
      "Final input to transformer (x_embed):\n",
      " tensor([[[ 2.0950, -0.5757, -0.0513, -0.0358,  1.3035, -2.3869,  0.4984,\n",
      "          -2.1776],\n",
      "         [-0.6449,  0.8153,  0.6986, -1.3857,  0.9781, -0.5495,  0.0054,\n",
      "          -0.9597],\n",
      "         [ 3.7360, -0.6269,  0.7099,  0.7106,  1.4430,  1.9519,  0.8210,\n",
      "           0.1322],\n",
      "         [ 0.3036,  0.3926, -0.9593,  0.2983, -1.0352,  0.7909,  1.8434,\n",
      "           0.8198]],\n",
      "\n",
      "        [[-0.6175, -1.8039,  0.7317, -1.0107, -0.1568, -1.6041, -0.3572,\n",
      "          -0.7217],\n",
      "         [-0.9313, -0.0432,  0.3352, -1.0585,  0.5202,  0.4894,  2.5880,\n",
      "          -1.5083],\n",
      "         [ 0.7129, -0.2989,  1.1737,  1.5863,  1.9783,  2.0282, -0.1684,\n",
      "           0.8802],\n",
      "         [-2.4870,  1.6536, -2.2006,  0.5034, -0.5721, -0.6087,  0.1548,\n",
      "           1.7364]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Hyperparams\n",
    "batch_size = 2\n",
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "vocab_size = 65\n",
    "\n",
    "# 2. Embedding tables\n",
    "token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "position_embedding_table = nn.Embedding(block_size, embedding_dim)\n",
    "\n",
    "# 3. Input tokens (2 sequences, 4 characters each)\n",
    "x = torch.tensor([\n",
    "    [12, 5, 8, 8],\n",
    "    [9, 1, 17, 33]\n",
    "])  # shape: (2, 4)\n",
    "\n",
    "# 4. Token embeddings (look up each token ID)\n",
    "token_emb = token_embedding_table(x)  # shape: (2, 4, 8)\n",
    "print(token_emb)\n",
    "# 5. Positional embeddings\n",
    "position_ids = torch.arange(block_size)  # [0, 1, 2, 3]\n",
    "pos_emb = position_embedding_table(position_ids)  # shape: (4, 8)\n",
    "pos_emb = pos_emb.unsqueeze(0)  # reshape to (1, 4, 8) to match batch\n",
    "\n",
    "# 6. Add them\n",
    "x_embed = token_emb + pos_emb  # shape: (2, 4, 8)\n",
    "\n",
    "print(\"Token embeddings:\\n\", token_emb)\n",
    "print(\"Positional embeddings:\\n\", pos_emb)\n",
    "print(\"Final input to transformer (x_embed):\\n\", x_embed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce21cac2-685d-4736-94b8-a8c25e8aad6a",
   "metadata": {},
   "source": [
    "# 3. Both are lookup tables\n",
    "# Token embedding table → vector for the type of token\n",
    "\n",
    "#  Positional embedding table → vector for the position of token\n",
    "\n",
    "#  Both are trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8f3f7-7d43-459c-bc35-c0cc6da2270a",
   "metadata": {},
   "source": [
    "### 🔁 Input Embedding Pipeline (with Example: \"help\")\n",
    "\n",
    "#### Input: \"help\"\n",
    "\n",
    "1. **Tokenization**  \n",
    "   - Character-level: ['h', 'e', 'l', 'p']  \n",
    "   - Token IDs (via vocab): [12, 5, 8, 9]  \n",
    "   - Shape: `(batch_size=1, block_size=4)`\n",
    "\n",
    "2. **Token Embedding**  \n",
    "   - Use `nn.Embedding(vocab_size, emb_dim)`  \n",
    "   - Maps each token ID to a learnable vector  \n",
    "   - Output shape: `(1, 4, emb_dim)`  \n",
    "   - Represents: *what* each token is\n",
    "\n",
    "3. **Positional Embedding**  \n",
    "   - Use `nn.Embedding(block_size, emb_dim)`  \n",
    "   - Creates a learnable vector for each position: [0, 1, 2, 3]  \n",
    "   - Output shape: `(1, 4, emb_dim)` (after unsqueeze)  \n",
    "   - Represents: *where* each token is in the sequence\n",
    "\n",
    "4. **Add Both Embeddings**  \n",
    "   - Final input: `token_emb + pos_emb`  \n",
    "   - Shape: `(1, 4, emb_dim)`  \n",
    "   - Each token now encodes both meaning and position\n",
    "\n",
    "✅ This combined embedding is passed to the first Transformer block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d919775-ae6c-45ce-84cc-30d99a86bf23",
   "metadata": {},
   "source": [
    "### 🔁 Project Embeddings into Query, Key, and Value (Q, K, V)\n",
    "\n",
    "To prepare for self-attention, each token's embedding is projected into 3 different vectors:\n",
    "\n",
    "- **Query (Q)** → What the token is looking for\n",
    "- **Key (K)**   → What the token offers to others\n",
    "- **Value (V)** → What the token contains to share\n",
    "\n",
    "These are created by applying 3 independent `nn.Linear` layers to the same input embedding:\n",
    "\n",
    "- Input: `x_embed` → shape `(batch_size, block_size, embedding_dim)`\n",
    "- Output:\n",
    "  - Q: `(batch_size, block_size, embedding_dim)`\n",
    "  - K: `(batch_size, block_size, embedding_dim)`\n",
    "  - V: `(batch_size, block_size, embedding_dim)`\n",
    "\n",
    "Each token is now ready to compare itself (via Q) to others (via K), and share information (via V).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0d8f7fe-4ef1-4d44-ba57-49bb7df49e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_embed shape: torch.Size([2, 4, 8])\n",
      "Q shape: torch.Size([2, 4, 8])\n",
      "K shape: torch.Size([2, 4, 8])\n",
      "V shape: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# --- Simulated inputs ---\n",
    "batch_size = 2\n",
    "block_size = 4\n",
    "embedding_dim = 8\n",
    "\n",
    "# Simulated token+pos embeddings (e.g. from nn.Embedding)\n",
    "x_embed = torch.randn(batch_size, block_size, embedding_dim)\n",
    "\n",
    "# --- Linear layers to create Q, K, V ---\n",
    "to_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "to_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "to_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "# --- Project to Q, K, V ---\n",
    "q = to_q(x_embed)\n",
    "k = to_k(x_embed)\n",
    "v = to_v(x_embed)\n",
    "\n",
    "# --- Check shapes ---\n",
    "print(\"x_embed shape:\", x_embed.shape)  # (2, 4, 8)\n",
    "print(\"Q shape:\", q.shape)              # (2, 4, 8)\n",
    "print(\"K shape:\", k.shape)              # (2, 4, 8)\n",
    "print(\"V shape:\", v.shape)              # (2, 4, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a755acb2-2500-4056-b8de-444e56d46a16",
   "metadata": {},
   "source": [
    "# 🔁 Transformers (in the Q/K/V step)\n",
    "# You're correct — we take one input (x_embed) and pass it through three completely separate linear layers:\n",
    "\n",
    "**1.Q = Linear_Q(x_embed)**\n",
    "\n",
    "**2.K = Linear_K(x_embed)**\n",
    "\n",
    "**3.V = Linear_V(x_embed)**\n",
    "\n",
    "# These 3 layers are not connected to each other\n",
    "\n",
    "# They do not pass values between themselves\n",
    "\n",
    "# They are just three different views of the same input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640de0c3-8372-41c8-9669-6f860ccc7eab",
   "metadata": {},
   "source": [
    "### 🔍 Q/K/V Linear Layers vs Classic ANN Layers\n",
    "\n",
    "In classic neural networks (ANNs or CNNs), layers are **stacked** — the output of one layer feeds into the next in a chain:  \n",
    "`Input → Hidden → Output`.\n",
    "\n",
    "But in Transformers, during the attention step, we apply **three separate linear projections** to the same input embedding:\n",
    "\n",
    "- `Query = Linear_Q(x_embed)`\n",
    "- `Key   = Linear_K(x_embed)`\n",
    "- `Value = Linear_V(x_embed)`\n",
    "\n",
    "These are:\n",
    "- **Not connected** to each other (no flow between them)\n",
    "- **Independent** layers that each produce a different role/view of the same token\n",
    "- Essential for enabling attention:  \n",
    "  → Q compares against K to decide \"who to look at\",  \n",
    "  → V is the content actually shared.\n",
    "\n",
    "This **branching structure** is a major architectural difference from traditional neural nets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76820c-445c-4623-bc00-6e5d21c405a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
