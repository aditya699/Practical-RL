{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d4b790-271e-4d20-9fb0-6b42f116e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q gymnasium torch matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cefd1-597f-4dcc-a801-7b1e43c24dfb",
   "metadata": {},
   "source": [
    "# Check if ur system has gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97dbc135-89e3-4629-bebf-981ade8eb470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA RTX 2000 Ada Generation\n",
      "GPU Memory: 16.8 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# If using GPU, print some info about it\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d0a95-0989-4eb5-8840-5defdba88a0a",
   "metadata": {},
   "source": [
    "# Load the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a52fc46-7e4b-46ef-b667-800b513fb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e4b2ea-2a50-457f-b6b7-d30d3f70db05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e89a808-9c7f-4343-a223-844a0457a149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "print(\"State shape:\", state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf55fb7-29f8-447d-9666-a80b6b10688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action space: Discrete(2)\n",
      "Initial observation: [ 0.04235871 -0.02958577 -0.0320276  -0.02347192]\n",
      "Observation shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment to get initial observation\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Initial observation:\", observation)\n",
    "print(\"Observation shape:\", observation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69e5a6-3bea-416e-9e27-652d2d788732",
   "metadata": {},
   "source": [
    "# Taking random actions over the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b487858-1dac-424f-b942-c0492523988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Taking 5 random steps ===\n",
      "Step 1:\n",
      "  Action taken: 0 (Left)\n",
      "  New observation: [-0.02022828 -0.15586087 -0.02850062  0.2861497 ]\n",
      "  Reward: 1.0\n",
      "  Episode ended: False\n",
      "\n",
      "Step 2:\n",
      "  Action taken: 1 (Right)\n",
      "  New observation: [-0.02334549  0.0396557  -0.02277763 -0.01538409]\n",
      "  Reward: 1.0\n",
      "  Episode ended: False\n",
      "\n",
      "Step 3:\n",
      "  Action taken: 0 (Left)\n",
      "  New observation: [-0.02255238 -0.15513231 -0.02308531  0.2700261 ]\n",
      "  Reward: 1.0\n",
      "  Episode ended: False\n",
      "\n",
      "Step 4:\n",
      "  Action taken: 1 (Right)\n",
      "  New observation: [-0.02565503  0.04031134 -0.01768479 -0.02984775]\n",
      "  Reward: 1.0\n",
      "  Episode ended: False\n",
      "\n",
      "Step 5:\n",
      "  Action taken: 1 (Right)\n",
      "  New observation: [-0.0248488   0.23568238 -0.01828174 -0.32805753]\n",
      "  Reward: 1.0\n",
      "  Episode ended: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(\"=== Taking 5 random steps ===\")\n",
    "for step in range(5):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()  # randomly choose 0 or 1\n",
    "    \n",
    "    # Execute the action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    print(f\"Step {step+1}:\")\n",
    "    print(f\"  Action taken: {action} ({'Left' if action == 0 else 'Right'})\")\n",
    "    print(f\"  New observation: {observation}\")\n",
    "    print(f\"  Reward: {reward}\")\n",
    "    print(f\"  Episode ended: {terminated or truncated}\")\n",
    "    print()\n",
    "    \n",
    "    # If episode ends, reset\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode ended! Resetting...\")\n",
    "        observation, info = env.reset()\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a0a6c-779b-4435-8446-9df80aece307",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------\n",
    "# ‚úÖ Deep Q-Learning Summary: What are we learning?\n",
    "#\n",
    "# - We are learning the Q-function: Q(s, a)\n",
    "#     ‚Üí Tells us: \"If I take action a in state s, how much total future reward can I expect?\"\n",
    "#\n",
    "# - The policy is not learned directly.\n",
    "#     ‚Üí At runtime, we pick the action with the highest Q-value:\n",
    "#         policy(s) = argmax_a Q(s, a)\n",
    "#\n",
    "# - Goal: Learn accurate Q-values so we can act optimally by just picking the best action.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cad1c2-1461-43b6-af91-1bf540db93e5",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìò Bellman Equation and Q-Value Learning (Value-Based RL)\n",
    "#\n",
    "# Richard Bellman introduced dynamic programming, which helps us\n",
    "# solve problems recursively by using solutions to subproblems.\n",
    "#\n",
    "# In Q-learning, we estimate the Q-value (expected return from\n",
    "# taking action 'a' in state 's') using the Bellman equation:\n",
    "#\n",
    "#     Q(s, a) ‚Üê r + Œ≥ * max_a' Q(s', a')\n",
    "#\n",
    "# Where:\n",
    "# - r is the immediate reward after taking action a in state s\n",
    "# - Œ≥ (gamma) is the discount factor for future rewards\n",
    "# - s' is the new state after taking action a\n",
    "# - max_a' Q(s', a') is the best future reward we can expect\n",
    "#\n",
    "# The goal: Learn good Q(s, a) values so that we can act using:\n",
    "#     policy(s) = argmax_a Q(s, a)\n",
    "#\n",
    "# This is the foundation of Value-Based Reinforcement Learning.\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5486e01-81f4-41d6-b34d-3e7ab0268ae1",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------\n",
    "# ‚ùó Why Tabular Q-Learning Doesn't Work for CartPole\n",
    "#\n",
    "# - The state in CartPole is a vector of 4 continuous values:\n",
    "#     [position, velocity, pole angle, pole angular velocity]\n",
    "#\n",
    "# - There are infinitely many possible states (real numbers),\n",
    "#   so we cannot build a Q-table with one row per state.\n",
    "#\n",
    "# ‚úÖ Solution: Use a Deep Neural Network (DQN)\n",
    "# - Input: the continuous state vector (4 floats)\n",
    "# - Output: Q-values for each possible action (e.g., [Q_left, Q_right])\n",
    "#\n",
    "# This lets us generalize across similar states using function approximation.\n",
    "# ------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
